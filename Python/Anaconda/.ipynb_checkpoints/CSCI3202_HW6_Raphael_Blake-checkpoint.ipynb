{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSCI 3202, Spring 2023\n",
    "# Homework 6\n",
    "# Due: Monday, May 1 at 9:00 pm\n",
    "\n",
    "In this assignment, I'm providing the solution and asking you to reflect on the Q-learning portion of the solution. The code is setup to run 3 trials, 10 steps each, and is hard-coded to start at the same location on each trial. It won't find the landing pad in only 3 trials, you will need to change that number if you want to see the code produce a valid solution. I added a few print statements in the Q-learning algorithm to help illustrate the code. Feel free to add as many additional print statements that you need to help you understand how its working.  \n",
    "\n",
    "<br>\n",
    "The questions are listed in Part 2 of this notebook.  This homework is worth 50 points\n",
    "<br> \n",
    "\n",
    "### Your name: Blake Raphael\n",
    "\n",
    "<br> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "# added packages\n",
    "import heapq\n",
    "from matplotlib import colors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1:  Reinforcement learning\n",
    "\n",
    "Consider a **cube** state space defined by $0 \\le x, y, z \\le L$. Suppose you are piloting/programming a drone to learn how to land on a platform at the center of the $z=0$ surface (the bottom). Some assumptions:\n",
    "* In this discrete world, if I say the drone is at $(x,y,z)$ I mean that it is in the box centered at $(x,y,z)$. And there are boxes (states) centered at $(x,y,z)$ for all $0 \\le x,y,z \\le L$. Each state is a 1 unit cube. So when $L=2$ (for example), there are cubes centered at each $x=0,1,2$, $y=0,1,2$ and so on, for a total state space size of $3^3 = 27$ states.\n",
    "* All of the states with $z=0$ are terminal states.\n",
    "* The state at the center of the bottom of the cubic state space is the landing pad. So, for example, when $L=4$, the landing pad is at $(x,y,z) = (2,2,0)$.\n",
    "* All terminal states ***except*** the landing pad have a reward of -1. The landing pad has a reward of +1.\n",
    "* All non-terminal states have a reward of -0.01.\n",
    "* The drone takes up exactly 1 cubic unit, and begins in a random non-terminal state.\n",
    "* The available actions in non-terminal states include moving exactly 1 unit Up (+z), Down (-z), North (+y), South (-y), East (+x) or West (-x). In a terminal state, the training episode should end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part A\n",
    "Write a class `MDPLanding` to represent the Markov decision process for this drone. Include methods for:\n",
    "1. `actions(state)`, which should return a list of all actions available from the given state\n",
    "2. `reward(state)`, which should return the reward for the given state\n",
    "3. `result(state, action)`, which should return the resulting state of doing the given action in the given state\n",
    "\n",
    "and attributes for:\n",
    "1. `states`, which is just a list of all the states in the state space, where each state is represented as an $(x,y,z)$ tuple\n",
    "2. `terminal_states`, a dictionary where keys are the terminal state tuples and the values are the rewards associated with those terminal states\n",
    "3. `default_reward`, which is a scalar for the reward associated with non-terminal states\n",
    "4. `all_actions`, a list of all possible actions (Up, Down, North, South, East, West)\n",
    "5. `discount`, the discount factor (use $\\gamma = 0.999$ for this entire problem)\n",
    "\n",
    "How you feed arguments/information into the class constructor is up to you.\n",
    "\n",
    "Note that actions are *deterministic* here.  The drone does not need to learn transition probabilities for outcomes of particular actions. What the drone does need to learn, however, is where the heck that landing pad is, and how to get there from any initial state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution:\n",
    "\n",
    "class MDPLanding:\n",
    "    def __init__(self, size, default_reward, actions, discount):\n",
    "        self.size = size\n",
    "        self.lz = (int(size/2), int(size/2), 0)\n",
    "        self.states = [(x,y,z) for x in range(size) for y in range(size) for z in range(size)]\n",
    "        self.terminal_states = {(x,y,0) : -1 for x in range(size) for y in range(size)}\n",
    "        self.terminal_states[self.lz] = +1\n",
    "        self.default_reward = default_reward\n",
    "        self.all_actions = actions\n",
    "        self.discount = discount\n",
    "\n",
    "    def actions(self, state):\n",
    "        '''all are available, unless you are in a terminal state\n",
    "        (the drone might bump into wall in some cases though)'''\n",
    "        if state in self.terminal_states:\n",
    "            return [None]\n",
    "        else:\n",
    "            return self.all_actions\n",
    "        \n",
    "    def reward(self, state):\n",
    "        return self.terminal_states[state] if state in self.terminal_states.keys() else self.default_reward\n",
    "                \n",
    "    def result(self, state, action):\n",
    "        '''result of doing `action` in `state`, deterministic.\n",
    "        `action` is one of N, S, E, W, U, D (as string)'''\n",
    "        \n",
    "        assert action in self.actions(state), 'Error: action needs to be available in that state'\n",
    "        assert state in self.states, 'Error: invalid state'\n",
    "        \n",
    "        if action=='N':\n",
    "            new_state = (state[0], state[1]+1, state[2])\n",
    "        elif action=='S':\n",
    "            new_state = (state[0], state[1]-1, state[2])\n",
    "        elif action=='E':\n",
    "            new_state = (state[0]+1, state[1], state[2])\n",
    "        elif action=='W':\n",
    "            new_state = (state[0]-1, state[1], state[2])\n",
    "        elif action=='U':\n",
    "            new_state = (state[0], state[1], state[2]+1)\n",
    "        elif action=='D':\n",
    "            new_state = (state[0], state[1], state[2]-1)\n",
    "        elif action is None:\n",
    "            return state\n",
    "        return new_state if new_state in self.states else state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Part B\n",
    "Write a function to implement **policy iteration** for this drone landing MDP. Create an MDP environment to represent the $L=4$ case (so 125 total states).\n",
    "\n",
    "Use your function to find an optimal policy for your new MDP environment. Check (by printing to screen) that the policy for the following states are what you expect, and comment on the results:\n",
    "1. $(2,2,1)$\n",
    "1. $(0,2,1)$\n",
    "1. $(2,0,1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy(2,2,1) = D\n",
      "Policy(0,2,1) = E\n",
      "Policy(2,0,1) = N\n"
     ]
    }
   ],
   "source": [
    "def policy_iteration(mdp):\n",
    "    # initilize utility for all states\n",
    "    utility = {s : 0 for s in mdp.states}\n",
    "    # initialize a policy for each state, being a random action\n",
    "    policy = {s: np.random.choice(mdp.actions(s)) for s in mdp.states}\n",
    "    while True:\n",
    "        # policy evaluation step\n",
    "        utility = policy_evaluation(policy, utility, mdp)\n",
    "        # policy improvement step\n",
    "        unchanged = True\n",
    "        for s in mdp.states:\n",
    "            best = (-999, None)\n",
    "            for a in mdp.actions(s):\n",
    "                new_util = utility[mdp.result(s,a)]\n",
    "                if new_util > best[0]:\n",
    "                    best = (new_util, a)\n",
    "            if best[1] != policy[s]:\n",
    "                policy[s] = best[1]\n",
    "                unchanged = False\n",
    "        if unchanged:\n",
    "            return policy\n",
    "\n",
    "def policy_evaluation(policy, utility, mdp, n_iter=50):\n",
    "    '''Do a handful (n_iter) of value iterations to update the\n",
    "    utility of each state, under the given policy'''\n",
    "    for i in range(n_iter):\n",
    "        for s in mdp.states:\n",
    "            if s in mdp.terminal_states:\n",
    "                utility[s] = mdp.reward(s)\n",
    "            else:\n",
    "                utility[s] = mdp.reward(s) + mdp.discount * utility[mdp.result(s, policy[s])]\n",
    "    return utility\n",
    "\n",
    "size = 5\n",
    "default_reward = -0.01\n",
    "discount = 0.999\n",
    "actions = ['N','S','E','W','U','D']\n",
    "mdp = MDPLanding(size, default_reward, actions, discount)\n",
    "\n",
    "policy = policy_iteration(mdp)\n",
    "print('Policy(2,2,1) = '+policy[(2,2,1)])\n",
    "print('Policy(0,2,1) = '+policy[(0,2,1)])\n",
    "print('Policy(2,0,1) = '+policy[(2,0,1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**\n",
    "\n",
    "Makes sense, because (2,2,1) is right above the landing pad in the 5x5x5 environment, so the drone should just go straight down. Similarly, (0,2,1) is West of the landing pad (lower x-value), so the drone needs to head toward +x, or East, and (2,0,1) is South of the landing pad (lower y-value), so the drone needs to head toward +y, or North."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part C\n",
    "\n",
    "Code up a **Q-learning** agent/algorithm to learn how to land the drone. You can do this however you like, as long as you use the MDP class structure defined above.  \n",
    "\n",
    "Your code should include some kind of a wrapper to run many trials to train the agent and learn the Q values (see Section 22.3 in the textbook - page 803 might be of particular interest).  You also do not need to have a separate function for the actual \"agent\"; your code can just be a \"for\" loop within which you are refining your estimate of the Q values.\n",
    "\n",
    "From each training trial, save the cumulative discounted reward (utility) over the course of that episode. That is, add up all of $\\gamma^t R(s_t)$ where the drone is in state $s_t$ during time step $t$, for the entire sequence. I refer to this as \"cumulative reward\" because we usually refer to \"utility\" as the utility *under an optimal policy*.\n",
    "\n",
    "Some guidelines:\n",
    "* The drone should initialize in a random non-terminal state for each new training episode.\n",
    "* The training episodes should be limited to 50 time steps, even if the drone has not yet landed. If the drone lands (in a terminal state), the training episode is over.\n",
    "* You may use whatever learning rate $\\alpha$ you decide is appropriate, and gives good results.\n",
    "* There are many forms of Q-learning. You can use whatever you would like, subject to the reliability targets in Part D below.\n",
    "* Your code should return:\n",
    "  * The learned Q values associated with each state-action pair.\n",
    "  * The cumulative reward for each training trial. \n",
    "  * Anything else that might be useful in the ensuing analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution:\n",
    "\n",
    "def run_many_trials(mdp, n_trials, n_steps=None):\n",
    "\n",
    "    Q = defaultdict(float)   # state-action matrix\n",
    "    Nsa = defaultdict(float) # state-action visit counter (exploration)\n",
    "    rewards = []\n",
    "    epsilon = 0.1 # for epsilon-greedy action\n",
    "    alpha = lambda n: 1./(1+n) # learning rate related to number of visits to this state\n",
    "    policy = {state : np.random.choice(mdp.all_actions) for state in mdp.states} # random initial policy\n",
    "        \n",
    "    for k in range(n_trials):\n",
    "        \n",
    "        #s = mdp.states[np.random.randint(len(mdp.states))] # random initial location\n",
    "        s = mdp.states[148] #seems like a nice starting place in the middle of the state space\n",
    "        print('mdp.states[148]:')\n",
    "        print(s)\n",
    "        print('trial: ', k)\n",
    "        while s in mdp.terminal_states:\n",
    "            s = mdp.states[np.random.randint(len(mdp.states))] # re-sample if terminal\n",
    "        r = mdp.reward(s)\n",
    "        cumulative_reward = 0\n",
    "                \n",
    "        for t in range(n_steps):\n",
    "\n",
    "            if s in mdp.terminal_states:\n",
    "                Q[s, None] = r\n",
    "                # update the policy\n",
    "                policy[s] = None\n",
    "                print('Terminal found:')\n",
    "                print(s)\n",
    "                cumulative_reward += (mdp.discount**t)*r\n",
    "                break\n",
    "            else:\n",
    "                # pick a new action and state\n",
    "                print('policy: ' + policy[s])\n",
    "                a = return_epsilon_greedy_action(policy, s, mdp, epsilon)\n",
    "                print(\"action: \" + a)\n",
    "                print(a)\n",
    "                s1 = mdp.result(s, a)\n",
    "                print(\"s1:\")\n",
    "                print(s1)\n",
    "                r1 = mdp.reward(s1)\n",
    "                print(\"r1:\")\n",
    "                print(r1)\n",
    "                # update the state-action \"matrix\"\n",
    "                Nsa[s, a] += 1\n",
    "                print(\"Q[s, a] before for s and a:\", Q[s,a], s, a)\n",
    "                Q[s, a] += alpha(Nsa[s, a]) * (r + mdp.discount * max(Q[s1, a1] for a1 in mdp.actions(s1)) - Q[s, a])\n",
    "                print(\"Q[s, a] for s and a:\", Q[s,a], s, a)\n",
    "                for a1 in mdp.actions(s1):\n",
    "                    print(\"Q for a1, s1\", Q[s1, a1], s1, a1)\n",
    "                #                Q[s, a] += 0.001 * (r + mdp.discount * max(Q[s1, a1] for a1 in mdp.actions(s1)) - Q[s, a])\n",
    "                # update the policy\n",
    "                best = (-999, None)\n",
    "                for a1 in mdp.actions(s):\n",
    "                    new = Q[s, a1]\n",
    "                    if new > best[0]:\n",
    "                        best = (new, a1)\n",
    "                policy[s] = best[1]\n",
    "                cumulative_reward += (mdp.discount**t)*r\n",
    "                s = s1\n",
    "                r = r1\n",
    "\n",
    "        rewards.append(cumulative_reward)\n",
    "\n",
    "    return Q, Nsa, rewards\n",
    "\n",
    "def return_epsilon_greedy_action(policy, state, mdp, epsilon=0.1):\n",
    "    '''Return a random action or the one with highest utility (so far)'''\n",
    "    if np.random.uniform(0, 1) <= epsilon:\n",
    "        action = np.random.choice(mdp.actions(state))\n",
    "    else:\n",
    "        action = policy[state]\n",
    "    return action\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part D\n",
    "\n",
    "Initialize the $L=10$ environment (so that the landing pad is at $(5,5,0)$). Run some number of training trials to train the drone.\n",
    "\n",
    "**How do I know if my drone is learned enough?**  If you take the mean cumulative reward across the last 5000 training trials, it should be around 0.80. This means at least about 10,000 (but probably more) training episodes will be necessary. It will take a few seconds on your computer, so start small to test your codes.\n",
    "\n",
    "**Then:** Compute block means of cumulative reward from all of your training trials. Use blocks of 500 training trials. This means you need to create some kind of array-like structure such that its first element is the mean of the first 500 trials' cumulative rewards; its second element is the mean of the 501-1000th trials' cumulative rewards; and so on. Make a plot of the block mean rewards as the training progresses. It should increase from about -0.5 initially to somewhere around +0.8.\n",
    "\n",
    "**And:** Print to the screen the mean of the last 5000 trials' cumulative rewards, to verify that it is indeed about 0.80."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mdp.states[148]:\n",
      "(1, 2, 5)\n",
      "trial:  0\n",
      "policy: N\n",
      "action: N\n",
      "N\n",
      "s1:\n",
      "(1, 3, 5)\n",
      "r1:\n",
      "-0.01\n",
      "Q[s, a] before for s and a: 0.0 (1, 2, 5) N\n",
      "Q[s, a] for s and a: -0.005 (1, 2, 5) N\n",
      "Q for a1, s1 0.0 (1, 3, 5) N\n",
      "Q for a1, s1 0.0 (1, 3, 5) S\n",
      "Q for a1, s1 0.0 (1, 3, 5) E\n",
      "Q for a1, s1 0.0 (1, 3, 5) W\n",
      "Q for a1, s1 0.0 (1, 3, 5) U\n",
      "Q for a1, s1 0.0 (1, 3, 5) D\n",
      "policy: D\n",
      "action: D\n",
      "D\n",
      "s1:\n",
      "(1, 3, 4)\n",
      "r1:\n",
      "-0.01\n",
      "Q[s, a] before for s and a: 0.0 (1, 3, 5) D\n",
      "Q[s, a] for s and a: -0.005 (1, 3, 5) D\n",
      "Q for a1, s1 0.0 (1, 3, 4) N\n",
      "Q for a1, s1 0.0 (1, 3, 4) S\n",
      "Q for a1, s1 0.0 (1, 3, 4) E\n",
      "Q for a1, s1 0.0 (1, 3, 4) W\n",
      "Q for a1, s1 0.0 (1, 3, 4) U\n",
      "Q for a1, s1 0.0 (1, 3, 4) D\n",
      "policy: N\n",
      "action: N\n",
      "N\n",
      "s1:\n",
      "(1, 4, 4)\n",
      "r1:\n",
      "-0.01\n",
      "Q[s, a] before for s and a: 0.0 (1, 3, 4) N\n",
      "Q[s, a] for s and a: -0.005 (1, 3, 4) N\n",
      "Q for a1, s1 0.0 (1, 4, 4) N\n",
      "Q for a1, s1 0.0 (1, 4, 4) S\n",
      "Q for a1, s1 0.0 (1, 4, 4) E\n",
      "Q for a1, s1 0.0 (1, 4, 4) W\n",
      "Q for a1, s1 0.0 (1, 4, 4) U\n",
      "Q for a1, s1 0.0 (1, 4, 4) D\n",
      "policy: D\n",
      "action: D\n",
      "D\n",
      "s1:\n",
      "(1, 4, 3)\n",
      "r1:\n",
      "-0.01\n",
      "Q[s, a] before for s and a: 0.0 (1, 4, 4) D\n",
      "Q[s, a] for s and a: -0.005 (1, 4, 4) D\n",
      "Q for a1, s1 0.0 (1, 4, 3) N\n",
      "Q for a1, s1 0.0 (1, 4, 3) S\n",
      "Q for a1, s1 0.0 (1, 4, 3) E\n",
      "Q for a1, s1 0.0 (1, 4, 3) W\n",
      "Q for a1, s1 0.0 (1, 4, 3) U\n",
      "Q for a1, s1 0.0 (1, 4, 3) D\n",
      "policy: D\n",
      "action: U\n",
      "U\n",
      "s1:\n",
      "(1, 4, 4)\n",
      "r1:\n",
      "-0.01\n",
      "Q[s, a] before for s and a: 0.0 (1, 4, 3) U\n",
      "Q[s, a] for s and a: -0.005 (1, 4, 3) U\n",
      "Q for a1, s1 0.0 (1, 4, 4) N\n",
      "Q for a1, s1 0.0 (1, 4, 4) S\n",
      "Q for a1, s1 0.0 (1, 4, 4) E\n",
      "Q for a1, s1 0.0 (1, 4, 4) W\n",
      "Q for a1, s1 0.0 (1, 4, 4) U\n",
      "Q for a1, s1 -0.005 (1, 4, 4) D\n",
      "policy: N\n",
      "action: N\n",
      "N\n",
      "s1:\n",
      "(1, 5, 4)\n",
      "r1:\n",
      "-0.01\n",
      "Q[s, a] before for s and a: 0.0 (1, 4, 4) N\n",
      "Q[s, a] for s and a: -0.005 (1, 4, 4) N\n",
      "Q for a1, s1 0.0 (1, 5, 4) N\n",
      "Q for a1, s1 0.0 (1, 5, 4) S\n",
      "Q for a1, s1 0.0 (1, 5, 4) E\n",
      "Q for a1, s1 0.0 (1, 5, 4) W\n",
      "Q for a1, s1 0.0 (1, 5, 4) U\n",
      "Q for a1, s1 0.0 (1, 5, 4) D\n",
      "policy: N\n",
      "action: N\n",
      "N\n",
      "s1:\n",
      "(1, 6, 4)\n",
      "r1:\n",
      "-0.01\n",
      "Q[s, a] before for s and a: 0.0 (1, 5, 4) N\n",
      "Q[s, a] for s and a: -0.005 (1, 5, 4) N\n",
      "Q for a1, s1 0.0 (1, 6, 4) N\n",
      "Q for a1, s1 0.0 (1, 6, 4) S\n",
      "Q for a1, s1 0.0 (1, 6, 4) E\n",
      "Q for a1, s1 0.0 (1, 6, 4) W\n",
      "Q for a1, s1 0.0 (1, 6, 4) U\n",
      "Q for a1, s1 0.0 (1, 6, 4) D\n",
      "policy: W\n",
      "action: W\n",
      "W\n",
      "s1:\n",
      "(0, 6, 4)\n",
      "r1:\n",
      "-0.01\n",
      "Q[s, a] before for s and a: 0.0 (1, 6, 4) W\n",
      "Q[s, a] for s and a: -0.005 (1, 6, 4) W\n",
      "Q for a1, s1 0.0 (0, 6, 4) N\n",
      "Q for a1, s1 0.0 (0, 6, 4) S\n",
      "Q for a1, s1 0.0 (0, 6, 4) E\n",
      "Q for a1, s1 0.0 (0, 6, 4) W\n",
      "Q for a1, s1 0.0 (0, 6, 4) U\n",
      "Q for a1, s1 0.0 (0, 6, 4) D\n",
      "policy: N\n",
      "action: N\n",
      "N\n",
      "s1:\n",
      "(0, 7, 4)\n",
      "r1:\n",
      "-0.01\n",
      "Q[s, a] before for s and a: 0.0 (0, 6, 4) N\n",
      "Q[s, a] for s and a: -0.005 (0, 6, 4) N\n",
      "Q for a1, s1 0.0 (0, 7, 4) N\n",
      "Q for a1, s1 0.0 (0, 7, 4) S\n",
      "Q for a1, s1 0.0 (0, 7, 4) E\n",
      "Q for a1, s1 0.0 (0, 7, 4) W\n",
      "Q for a1, s1 0.0 (0, 7, 4) U\n",
      "Q for a1, s1 0.0 (0, 7, 4) D\n",
      "policy: E\n",
      "action: E\n",
      "E\n",
      "s1:\n",
      "(1, 7, 4)\n",
      "r1:\n",
      "-0.01\n",
      "Q[s, a] before for s and a: 0.0 (0, 7, 4) E\n",
      "Q[s, a] for s and a: -0.005 (0, 7, 4) E\n",
      "Q for a1, s1 0.0 (1, 7, 4) N\n",
      "Q for a1, s1 0.0 (1, 7, 4) S\n",
      "Q for a1, s1 0.0 (1, 7, 4) E\n",
      "Q for a1, s1 0.0 (1, 7, 4) W\n",
      "Q for a1, s1 0.0 (1, 7, 4) U\n",
      "Q for a1, s1 0.0 (1, 7, 4) D\n",
      "mdp.states[148]:\n",
      "(1, 2, 5)\n",
      "trial:  1\n",
      "policy: S\n",
      "action: S\n",
      "S\n",
      "s1:\n",
      "(1, 1, 5)\n",
      "r1:\n",
      "-0.01\n",
      "Q[s, a] before for s and a: 0.0 (1, 2, 5) S\n",
      "Q[s, a] for s and a: -0.005 (1, 2, 5) S\n",
      "Q for a1, s1 0.0 (1, 1, 5) N\n",
      "Q for a1, s1 0.0 (1, 1, 5) S\n",
      "Q for a1, s1 0.0 (1, 1, 5) E\n",
      "Q for a1, s1 0.0 (1, 1, 5) W\n",
      "Q for a1, s1 0.0 (1, 1, 5) U\n",
      "Q for a1, s1 0.0 (1, 1, 5) D\n",
      "policy: W\n",
      "action: W\n",
      "W\n",
      "s1:\n",
      "(0, 1, 5)\n",
      "r1:\n",
      "-0.01\n",
      "Q[s, a] before for s and a: 0.0 (1, 1, 5) W\n",
      "Q[s, a] for s and a: -0.005 (1, 1, 5) W\n",
      "Q for a1, s1 0.0 (0, 1, 5) N\n",
      "Q for a1, s1 0.0 (0, 1, 5) S\n",
      "Q for a1, s1 0.0 (0, 1, 5) E\n",
      "Q for a1, s1 0.0 (0, 1, 5) W\n",
      "Q for a1, s1 0.0 (0, 1, 5) U\n",
      "Q for a1, s1 0.0 (0, 1, 5) D\n",
      "policy: U\n",
      "action: U\n",
      "U\n",
      "s1:\n",
      "(0, 1, 6)\n",
      "r1:\n",
      "-0.01\n",
      "Q[s, a] before for s and a: 0.0 (0, 1, 5) U\n",
      "Q[s, a] for s and a: -0.005 (0, 1, 5) U\n",
      "Q for a1, s1 0.0 (0, 1, 6) N\n",
      "Q for a1, s1 0.0 (0, 1, 6) S\n",
      "Q for a1, s1 0.0 (0, 1, 6) E\n",
      "Q for a1, s1 0.0 (0, 1, 6) W\n",
      "Q for a1, s1 0.0 (0, 1, 6) U\n",
      "Q for a1, s1 0.0 (0, 1, 6) D\n",
      "policy: W\n",
      "action: W\n",
      "W\n",
      "s1:\n",
      "(0, 1, 6)\n",
      "r1:\n",
      "-0.01\n",
      "Q[s, a] before for s and a: 0.0 (0, 1, 6) W\n",
      "Q[s, a] for s and a: -0.005 (0, 1, 6) W\n",
      "Q for a1, s1 0.0 (0, 1, 6) N\n",
      "Q for a1, s1 0.0 (0, 1, 6) S\n",
      "Q for a1, s1 0.0 (0, 1, 6) E\n",
      "Q for a1, s1 -0.005 (0, 1, 6) W\n",
      "Q for a1, s1 0.0 (0, 1, 6) U\n",
      "Q for a1, s1 0.0 (0, 1, 6) D\n",
      "policy: N\n",
      "action: N\n",
      "N\n",
      "s1:\n",
      "(0, 2, 6)\n",
      "r1:\n",
      "-0.01\n",
      "Q[s, a] before for s and a: 0.0 (0, 1, 6) N\n",
      "Q[s, a] for s and a: -0.005 (0, 1, 6) N\n",
      "Q for a1, s1 0.0 (0, 2, 6) N\n",
      "Q for a1, s1 0.0 (0, 2, 6) S\n",
      "Q for a1, s1 0.0 (0, 2, 6) E\n",
      "Q for a1, s1 0.0 (0, 2, 6) W\n",
      "Q for a1, s1 0.0 (0, 2, 6) U\n",
      "Q for a1, s1 0.0 (0, 2, 6) D\n",
      "policy: S\n",
      "action: W\n",
      "W\n",
      "s1:\n",
      "(0, 2, 6)\n",
      "r1:\n",
      "-0.01\n",
      "Q[s, a] before for s and a: 0.0 (0, 2, 6) W\n",
      "Q[s, a] for s and a: -0.005 (0, 2, 6) W\n",
      "Q for a1, s1 0.0 (0, 2, 6) N\n",
      "Q for a1, s1 0.0 (0, 2, 6) S\n",
      "Q for a1, s1 0.0 (0, 2, 6) E\n",
      "Q for a1, s1 -0.005 (0, 2, 6) W\n",
      "Q for a1, s1 0.0 (0, 2, 6) U\n",
      "Q for a1, s1 0.0 (0, 2, 6) D\n",
      "policy: N\n",
      "action: N\n",
      "N\n",
      "s1:\n",
      "(0, 3, 6)\n",
      "r1:\n",
      "-0.01\n",
      "Q[s, a] before for s and a: 0.0 (0, 2, 6) N\n",
      "Q[s, a] for s and a: -0.005 (0, 2, 6) N\n",
      "Q for a1, s1 0.0 (0, 3, 6) N\n",
      "Q for a1, s1 0.0 (0, 3, 6) S\n",
      "Q for a1, s1 0.0 (0, 3, 6) E\n",
      "Q for a1, s1 0.0 (0, 3, 6) W\n",
      "Q for a1, s1 0.0 (0, 3, 6) U\n",
      "Q for a1, s1 0.0 (0, 3, 6) D\n",
      "policy: D\n",
      "action: D\n",
      "D\n",
      "s1:\n",
      "(0, 3, 5)\n",
      "r1:\n",
      "-0.01\n",
      "Q[s, a] before for s and a: 0.0 (0, 3, 6) D\n",
      "Q[s, a] for s and a: -0.005 (0, 3, 6) D\n",
      "Q for a1, s1 0.0 (0, 3, 5) N\n",
      "Q for a1, s1 0.0 (0, 3, 5) S\n",
      "Q for a1, s1 0.0 (0, 3, 5) E\n",
      "Q for a1, s1 0.0 (0, 3, 5) W\n",
      "Q for a1, s1 0.0 (0, 3, 5) U\n",
      "Q for a1, s1 0.0 (0, 3, 5) D\n",
      "policy: U\n",
      "action: U\n",
      "U\n",
      "s1:\n",
      "(0, 3, 6)\n",
      "r1:\n",
      "-0.01\n",
      "Q[s, a] before for s and a: 0.0 (0, 3, 5) U\n",
      "Q[s, a] for s and a: -0.005 (0, 3, 5) U\n",
      "Q for a1, s1 0.0 (0, 3, 6) N\n",
      "Q for a1, s1 0.0 (0, 3, 6) S\n",
      "Q for a1, s1 0.0 (0, 3, 6) E\n",
      "Q for a1, s1 0.0 (0, 3, 6) W\n",
      "Q for a1, s1 0.0 (0, 3, 6) U\n",
      "Q for a1, s1 -0.005 (0, 3, 6) D\n",
      "policy: N\n",
      "action: N\n",
      "N\n",
      "s1:\n",
      "(0, 4, 6)\n",
      "r1:\n",
      "-0.01\n",
      "Q[s, a] before for s and a: 0.0 (0, 3, 6) N\n",
      "Q[s, a] for s and a: -0.005 (0, 3, 6) N\n",
      "Q for a1, s1 0.0 (0, 4, 6) N\n",
      "Q for a1, s1 0.0 (0, 4, 6) S\n",
      "Q for a1, s1 0.0 (0, 4, 6) E\n",
      "Q for a1, s1 0.0 (0, 4, 6) W\n",
      "Q for a1, s1 0.0 (0, 4, 6) U\n",
      "Q for a1, s1 0.0 (0, 4, 6) D\n",
      "mdp.states[148]:\n",
      "(1, 2, 5)\n",
      "trial:  2\n",
      "policy: E\n",
      "action: E\n",
      "E\n",
      "s1:\n",
      "(2, 2, 5)\n",
      "r1:\n",
      "-0.01\n",
      "Q[s, a] before for s and a: 0.0 (1, 2, 5) E\n",
      "Q[s, a] for s and a: -0.005 (1, 2, 5) E\n",
      "Q for a1, s1 0.0 (2, 2, 5) N\n",
      "Q for a1, s1 0.0 (2, 2, 5) S\n",
      "Q for a1, s1 0.0 (2, 2, 5) E\n",
      "Q for a1, s1 0.0 (2, 2, 5) W\n",
      "Q for a1, s1 0.0 (2, 2, 5) U\n",
      "Q for a1, s1 0.0 (2, 2, 5) D\n",
      "policy: N\n",
      "action: N\n",
      "N\n",
      "s1:\n",
      "(2, 3, 5)\n",
      "r1:\n",
      "-0.01\n",
      "Q[s, a] before for s and a: 0.0 (2, 2, 5) N\n",
      "Q[s, a] for s and a: -0.005 (2, 2, 5) N\n",
      "Q for a1, s1 0.0 (2, 3, 5) N\n",
      "Q for a1, s1 0.0 (2, 3, 5) S\n",
      "Q for a1, s1 0.0 (2, 3, 5) E\n",
      "Q for a1, s1 0.0 (2, 3, 5) W\n",
      "Q for a1, s1 0.0 (2, 3, 5) U\n",
      "Q for a1, s1 0.0 (2, 3, 5) D\n",
      "policy: E\n",
      "action: E\n",
      "E\n",
      "s1:\n",
      "(3, 3, 5)\n",
      "r1:\n",
      "-0.01\n",
      "Q[s, a] before for s and a: 0.0 (2, 3, 5) E\n",
      "Q[s, a] for s and a: -0.005 (2, 3, 5) E\n",
      "Q for a1, s1 0.0 (3, 3, 5) N\n",
      "Q for a1, s1 0.0 (3, 3, 5) S\n",
      "Q for a1, s1 0.0 (3, 3, 5) E\n",
      "Q for a1, s1 0.0 (3, 3, 5) W\n",
      "Q for a1, s1 0.0 (3, 3, 5) U\n",
      "Q for a1, s1 0.0 (3, 3, 5) D\n",
      "policy: D\n",
      "action: D\n",
      "D\n",
      "s1:\n",
      "(3, 3, 4)\n",
      "r1:\n",
      "-0.01\n",
      "Q[s, a] before for s and a: 0.0 (3, 3, 5) D\n",
      "Q[s, a] for s and a: -0.005 (3, 3, 5) D\n",
      "Q for a1, s1 0.0 (3, 3, 4) N\n",
      "Q for a1, s1 0.0 (3, 3, 4) S\n",
      "Q for a1, s1 0.0 (3, 3, 4) E\n",
      "Q for a1, s1 0.0 (3, 3, 4) W\n",
      "Q for a1, s1 0.0 (3, 3, 4) U\n",
      "Q for a1, s1 0.0 (3, 3, 4) D\n",
      "policy: W\n",
      "action: W\n",
      "W\n",
      "s1:\n",
      "(2, 3, 4)\n",
      "r1:\n",
      "-0.01\n",
      "Q[s, a] before for s and a: 0.0 (3, 3, 4) W\n",
      "Q[s, a] for s and a: -0.005 (3, 3, 4) W\n",
      "Q for a1, s1 0.0 (2, 3, 4) N\n",
      "Q for a1, s1 0.0 (2, 3, 4) S\n",
      "Q for a1, s1 0.0 (2, 3, 4) E\n",
      "Q for a1, s1 0.0 (2, 3, 4) W\n",
      "Q for a1, s1 0.0 (2, 3, 4) U\n",
      "Q for a1, s1 0.0 (2, 3, 4) D\n",
      "policy: W\n",
      "action: W\n",
      "W\n",
      "s1:\n",
      "(1, 3, 4)\n",
      "r1:\n",
      "-0.01\n",
      "Q[s, a] before for s and a: 0.0 (2, 3, 4) W\n",
      "Q[s, a] for s and a: -0.005 (2, 3, 4) W\n",
      "Q for a1, s1 -0.005 (1, 3, 4) N\n",
      "Q for a1, s1 0.0 (1, 3, 4) S\n",
      "Q for a1, s1 0.0 (1, 3, 4) E\n",
      "Q for a1, s1 0.0 (1, 3, 4) W\n",
      "Q for a1, s1 0.0 (1, 3, 4) U\n",
      "Q for a1, s1 0.0 (1, 3, 4) D\n",
      "policy: S\n",
      "action: S\n",
      "S\n",
      "s1:\n",
      "(1, 2, 4)\n",
      "r1:\n",
      "-0.01\n",
      "Q[s, a] before for s and a: 0.0 (1, 3, 4) S\n",
      "Q[s, a] for s and a: -0.005 (1, 3, 4) S\n",
      "Q for a1, s1 0.0 (1, 2, 4) N\n",
      "Q for a1, s1 0.0 (1, 2, 4) S\n",
      "Q for a1, s1 0.0 (1, 2, 4) E\n",
      "Q for a1, s1 0.0 (1, 2, 4) W\n",
      "Q for a1, s1 0.0 (1, 2, 4) U\n",
      "Q for a1, s1 0.0 (1, 2, 4) D\n",
      "policy: N\n",
      "action: N\n",
      "N\n",
      "s1:\n",
      "(1, 3, 4)\n",
      "r1:\n",
      "-0.01\n",
      "Q[s, a] before for s and a: 0.0 (1, 2, 4) N\n",
      "Q[s, a] for s and a: -0.005 (1, 2, 4) N\n",
      "Q for a1, s1 -0.005 (1, 3, 4) N\n",
      "Q for a1, s1 -0.005 (1, 3, 4) S\n",
      "Q for a1, s1 0.0 (1, 3, 4) E\n",
      "Q for a1, s1 0.0 (1, 3, 4) W\n",
      "Q for a1, s1 0.0 (1, 3, 4) U\n",
      "Q for a1, s1 0.0 (1, 3, 4) D\n",
      "policy: E\n",
      "action: E\n",
      "E\n",
      "s1:\n",
      "(2, 3, 4)\n",
      "r1:\n",
      "-0.01\n",
      "Q[s, a] before for s and a: 0.0 (1, 3, 4) E\n",
      "Q[s, a] for s and a: -0.005 (1, 3, 4) E\n",
      "Q for a1, s1 0.0 (2, 3, 4) N\n",
      "Q for a1, s1 0.0 (2, 3, 4) S\n",
      "Q for a1, s1 0.0 (2, 3, 4) E\n",
      "Q for a1, s1 -0.005 (2, 3, 4) W\n",
      "Q for a1, s1 0.0 (2, 3, 4) U\n",
      "Q for a1, s1 0.0 (2, 3, 4) D\n",
      "policy: N\n",
      "action: N\n",
      "N\n",
      "s1:\n",
      "(2, 4, 4)\n",
      "r1:\n",
      "-0.01\n",
      "Q[s, a] before for s and a: 0.0 (2, 3, 4) N\n",
      "Q[s, a] for s and a: -0.005 (2, 3, 4) N\n",
      "Q for a1, s1 0.0 (2, 4, 4) N\n",
      "Q for a1, s1 0.0 (2, 4, 4) S\n",
      "Q for a1, s1 0.0 (2, 4, 4) E\n",
      "Q for a1, s1 0.0 (2, 4, 4) W\n",
      "Q for a1, s1 0.0 (2, 4, 4) U\n",
      "Q for a1, s1 0.0 (2, 4, 4) D\n"
     ]
    }
   ],
   "source": [
    "# Solution Part D:\n",
    "\n",
    "size = 11\n",
    "default_reward = -0.01\n",
    "discount = 0.999\n",
    "actions = ['N','S','E','W','U','D']\n",
    "mdp = MDPLanding(size, default_reward, actions, discount)\n",
    "n_trials = 3 #homework 3, part 2\n",
    "Q,Nsa,rewards = run_many_trials(mdp, n_trials, n_steps=10)\n",
    "#Q,Nsa,rewards = run_many_trials(mdp, n_trials=1, n_steps=50)\n",
    "\n",
    "\n",
    "#di = 500\n",
    "#rewards_block = list(np.mean(np.asarray(rewards).reshape(-1, di),1))\n",
    "#plt.plot(list(range(0,len(rewards),di)), rewards_block)\n",
    "#plt.xlabel('Training episode')\n",
    "#plt.ylabel('Cumulative reward')\n",
    "#plt.show()\n",
    "\n",
    "#print('Last 5000 trials mean cumulative reward = {:0.4f}'.format(np.mean(rewards[-5000:])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Homework 3 questions.\n",
    "These questions refer to the Q-learning implementation in Parts C and D of this notebook. \n",
    "\n",
    "**1:** (5 pts) This code in Part D is set up to run 3 trials, 10 steps each, starting from (1,2,5). The values for state, reward, policy, and Q[s,a] print on each step. \n",
    "- What is the first action taken and what state does the agent move to? \n",
    "- The values for policy and action are printed. What's the difference in the code?\n",
    "- What are the lines of code that implement the action and determine the new state?\n",
    "\n",
    "**2:** (5 pts) How is the s1 variable used in the code, including in the Q[s,a] calculation?\n",
    "\n",
    "**3:** (5 pts) What are the initial values for the Q-matrix, the discount factor, learning rate, and the policy matrix?\n",
    "\n",
    "**4:** (10 pts) What is the purpose of the *max(Q[s1, a1] for a1 in mdp.actions(s1))* code in the Q[s,a] calculation? How does it influence the Q[s,a] value?\n",
    "\n",
    "**5:** (10 pts) Where does the code store the policy for each state? How is it updated and where is it used in the code?\n",
    "\n",
    "**6:** (15 pts) Write a paragram explaining how the algorithm learns in this example.  What are the limitations of this method?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#1: The first action taken is to move North and the state the agent moves to is (1,3,5). The difference in the code is change in policy used, as each trial takes a different starting action, and the policies for states are changed from trial to trial. The lines of code that implement the action and determine the new state are lines 36 through 42 in the Part C block of code.\n",
    "\n",
    "#2: The s1 variable, including in the Q[s,a] calculation, is used for determining the next state given the policy of the action to take in our current state. We update s to be s1 before using it in our next Q[s,a] calculation which helps us determine our reward values.\n",
    "\n",
    "#3: The initial values for the Q-matrix are all 0.0 as it is intialized to a default dictionary with no reward values. The initial value for the discount factor is 0.999. The initial value for learning rate is 1 as we want to encourage moving states, but as we visit more states, our learning rate decreases (as shown by the 1 / (1 + n) where n is the number of states we have visited). The initial values for the policy matrix are a random choice of North, East, South, West, Up, or Down.\n",
    "\n",
    "#4: The purpose of the *max(Q[s1, a1] for a1 in mdp.actions(s1))* code in the Q[s,a] calculation is to maximize our reward value for the available actions we have in our state s1. It influences the Q[s,a] value by making it as large as possible for the given state.\n",
    "\n",
    "#5: The code stores the policy for each state in a dictionary that links each state to a policy at random initially. It is updated when we find a better policy in regards to the reward gained for specific actions related to that state. In short, if we find an action that gives us a better reward than our current policy (i.e. the line that checks if new > best[0]), then we update that policy. It is used in our return_epsilon_greedy_action function to determine which action we should take. It is also used in our run_many_trials code when we want to update the policy or check if we want to update the policy.\n",
    "\n",
    "#6: The algorithim learns in this example by using a greedy approach. The limitations are that it will usually pick the best reward for the very next action, potentially missing out on other longer paths that are shorter. That leaves us with a potentially short-sighted agent that misses out on a better overall path due to focusing on capturing the best rewards early on in its learning phase. As well as this, future trials can be negatively impacted by the greedy policy changes made when we were in our learning phase, like the example with the slot machines in class showed us. Ultimately this can lead our agent astray and allow us to fail when trying to find our overall goal due to our misguided policies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
